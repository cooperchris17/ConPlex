{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of Stanza models with country codes is available at  \n",
    "https://stanfordnlp.github.io/stanza/performance.html  \n",
    "Install the model the first time that you use it  \n",
    "The rest of the notebook wil run for whatever language you input in language_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code = \"it\" # input the language code here\n",
    "\n",
    "# I downloaded the models in the current directory, you can change this if you want to downlod the model in a different place\n",
    "directory = os.getcwd()\n",
    "\n",
    "# remove the hashtag on the line below to download the model (the first time you use it)\n",
    "# stanza.download(language_code, model_dir=directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate <u>diversity</u> (Shannon's entropy) of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def sentence_diversity_calc(tags):\n",
    "  \"\"\"Calculates the Shannon entropy of a list of POS tag pairs.\n",
    "\n",
    "  Args:\n",
    "    pairs: A list of POS tag pairs (tuples).\n",
    "\n",
    "  Returns:\n",
    "    The Shannon entropy of the pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  pairs = []\n",
    "  for i in range(len(tags) - 1):\n",
    "    pairs.append((tags[i], tags[i+1]))\n",
    "  \n",
    "  # Count occurrences of each pair\n",
    "  pair_counts = {}\n",
    "  for pair in pairs:\n",
    "    pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "\n",
    "  # Calculate probabilities\n",
    "  total_pairs = sum(pair_counts.values())\n",
    "  probabilities = [count / total_pairs for count in pair_counts.values()]\n",
    "\n",
    "  # Calculate entropy using SciPy's entropy function\n",
    "  H = stats.entropy(probabilities, base=2)  # Base 2 for bits\n",
    "\n",
    "  return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to caclulate the <u>productivity</u> of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def sentence_productivity_calc(words, tags):\n",
    "  \"\"\"Calculates the productivity of a sentence based on word and tag data.\n",
    "\n",
    "  Args:\n",
    "    words: A list of words in the sentence.\n",
    "    tags: A list of corresponding POS tags.\n",
    "\n",
    "  Returns:\n",
    "    The productivity of the sentence.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate joint entropy H(W, T)\n",
    "  word_tag_pairs = list(zip(words, tags))\n",
    "  pair_counts = {}\n",
    "  for pair in word_tag_pairs:\n",
    "    pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "  total_pairs = sum(pair_counts.values())\n",
    "  probabilities = [count / total_pairs for count in pair_counts.values()]\n",
    "  H_WT = stats.entropy(probabilities, base=2)\n",
    "\n",
    "  # Calculate entropy of tags H(T)\n",
    "  tag_counts = {}\n",
    "  for tag in tags:\n",
    "    tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "  total_tags = sum(tag_counts.values())\n",
    "  tag_probabilities = [count / total_tags for count in tag_counts.values()]\n",
    "  H_T = stats.entropy(tag_probabilities, base=2)\n",
    "\n",
    "  # Calculate conditional entropy H(W | T)\n",
    "  H_WT_given_T = H_WT - H_T\n",
    "\n",
    "  # Calculate productivity\n",
    "  productivity = H_WT_given_T + 1\n",
    "\n",
    "  return productivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the document complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_complexity_calc(sentences, doc):\n",
    "    \"\"\"Calculates the document complexity, diversity, and productivity.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences (each sentence as a list of words).\n",
    "        doc: The entire document object.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - Document complexity\n",
    "        - Average document diversity\n",
    "        - Average document productivity\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(sentences)\n",
    "    total_complexity = 0\n",
    "    total_diversity = 0\n",
    "    total_productivity = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sen_words = []\n",
    "        sen_pos = []\n",
    "\n",
    "        for word in sentence.words:\n",
    "            if word.upos != \"PUNCT\":\n",
    "                sen_words.append(word.text.lower())\n",
    "                sen_pos.append(word.xpos)\n",
    "\n",
    "        # Calculate D(si) and P(si) for the current sentence\n",
    "        diversity = sentence_diversity_calc(sen_pos)\n",
    "        productivity = sentence_productivity_calc(sen_words, sen_pos)\n",
    "\n",
    "        # Update totals\n",
    "        total_complexity += diversity * productivity\n",
    "        total_diversity += diversity\n",
    "        total_productivity += productivity\n",
    "\n",
    "    # Calculate averages\n",
    "    document_complexity = total_complexity / N\n",
    "    average_diversity = total_diversity / N\n",
    "    average_productivity = total_productivity / N\n",
    "\n",
    "    return document_complexity, average_diversity, average_productivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the text(s) and process it using ipywidgets buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 20:02:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5cf8388ab442a1b0ce58300ca20004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 20:02:30 INFO: Downloaded file to c:\\Users\\admin\\Documents\\Research\\Nelson_Complexity\\other_languages\\Merlin_Corpus\\resources.json\n",
      "2025-01-30 20:02:31 INFO: Loading these models for language: it (Italian):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | vit_charlm        |\n",
      "| depparse     | combined_charlm   |\n",
      "| ner          | fbk               |\n",
      "====================================\n",
      "\n",
      "2025-01-30 20:02:31 INFO: Using device: cpu\n",
      "2025-01-30 20:02:31 INFO: Loading: tokenize\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-30 20:02:32 INFO: Loading: mwt\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-30 20:02:32 INFO: Loading: pos\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-30 20:02:32 INFO: Loading: lemma\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-30 20:02:32 INFO: Loading: constituency\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\constituency\\base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-30 20:02:33 INFO: Loading: depparse\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-30 20:02:33 INFO: Loading: ner\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\ner\\trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-30 20:02:34 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7e078736b7447e88474f138b86242c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h1>Construction Complexity Calculator</h1>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761a1c3802b54f16a3dfc004eae845d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"Input text in the box and click 'Process Text Input', or click 'Upload' to upload and process .txt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44ca61828bb45a0922ba200b0e91240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Input text:', layout=Layout(height='300px', width='600px'), placeholder='Copy …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ff3b15b00244b0b682c5b2e8b96ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process Text Input', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4190e0ca171c4bd998310859d9f7cd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.txt', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081ba265e3a547e89c58dfea2168a2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc29e0c6c19340bebf9e69c449c07ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Progress:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Stanza pipeline\n",
    "nlp = stanza.Pipeline(language_code, model_dir=directory)\n",
    "\n",
    "# Create a title widget\n",
    "title_widget = widgets.HTML(value=\"<h1>Construction Complexity Calculator</h1>\")\n",
    "\n",
    "# Create a combined instructions widget\n",
    "instructions_widget = widgets.HTML(value=\"Input text in the box and click 'Process Text Input', or click 'Upload' to upload and process .txt files.\")\n",
    "\n",
    "# Create a text box widget\n",
    "text_box_widget = widgets.Textarea(\n",
    "    placeholder='Copy and paste text here',\n",
    "    description='Input text:',\n",
    "    layout=widgets.Layout(height='300px', width='600px')\n",
    ")\n",
    "\n",
    "# Create a file upload widget\n",
    "file_upload_widget = widgets.FileUpload(\n",
    "    accept=\".txt\",  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=True  # True to accept multiple files upload else False\n",
    ")\n",
    "\n",
    "# Create an output widget to display the result\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Create a progress bar widget\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='Progress:',\n",
    "    bar_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Define the function to process the input files and create a CSV file\n",
    "def process_input(files=None, text=None):\n",
    "    output_widget.append_stdout(\"\\nProcessing...this may take some time!\\n\")\n",
    "    \n",
    "    if files:\n",
    "        results = []\n",
    "        total_files = len(files)\n",
    "        progress_bar.max = total_files\n",
    "        \n",
    "        for i, uploaded_file in enumerate(files):\n",
    "            content = bytes(uploaded_file['content']).decode('utf-8')\n",
    "            doc = nlp(content)\n",
    "            complexity, avg_diversity, avg_productivity = document_complexity_calc(doc.sentences, doc)\n",
    "            results.append({'filename': uploaded_file['name'],\n",
    "                            'complexity': complexity,\n",
    "                            'diversity': avg_diversity,\n",
    "                            'productivity': avg_productivity})\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.value = i + 1\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        csv_filename = 'complexity_scores.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        progress_bar.value = 0  # Reset progress bar\n",
    "    \n",
    "    elif text:\n",
    "        progress_bar.max = 1\n",
    "        progress_bar.value = 0\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        complexity, avg_diversity, avg_productivity = document_complexity_calc(doc.sentences, doc)\n",
    "        \n",
    "        progress_bar.value = 1  # Complete progress bar\n",
    "        progress_bar.value = 0  # Reset progress bar\n",
    "\n",
    "    # Add the output message\n",
    "    output_widget.append_stdout(f\"\\nComplexity scores saved to {csv_filename}\" if files else f\"\"\"\n",
    "        Complexity score: {complexity}\n",
    "        Diversity: {avg_diversity}\n",
    "        Productivity: {avg_productivity}\n",
    "        \"\"\")\n",
    "\n",
    "# Attach a button to process the text input\n",
    "process_text_button = widgets.Button(description=\"Process Text Input\")\n",
    "process_text_button.on_click(lambda b: process_input(text=text_box_widget.value))\n",
    "\n",
    "# Automatically process files when they are uploaded\n",
    "def on_file_upload(change):\n",
    "    process_input(files=change['new'])\n",
    "\n",
    "file_upload_widget.observe(on_file_upload, names='value')\n",
    "\n",
    "# Display the widgets\n",
    "display(title_widget, instructions_widget, text_box_widget, process_text_button, file_upload_widget, output_widget, progress_bar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
