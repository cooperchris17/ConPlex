{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is better because it also outputs the diversity and productivity (for each document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate <u>diversity</u> (Shannon's entropy) of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def sentence_diversity_calc(tags):\n",
    "  \"\"\"Calculates the Shannon entropy of a list of POS tag pairs.\n",
    "\n",
    "  Args:\n",
    "    pairs: A list of POS tag pairs (tuples).\n",
    "\n",
    "  Returns:\n",
    "    The Shannon entropy of the pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  pairs = []\n",
    "  for i in range(len(tags) - 1):\n",
    "    pairs.append((tags[i], tags[i+1]))\n",
    "  \n",
    "  # Count occurrences of each pair\n",
    "  pair_counts = {}\n",
    "  for pair in pairs:\n",
    "    pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "\n",
    "  # Calculate probabilities\n",
    "  total_pairs = sum(pair_counts.values())\n",
    "  probabilities = [count / total_pairs for count in pair_counts.values()]\n",
    "\n",
    "  # Calculate entropy using SciPy's entropy function\n",
    "  H = stats.entropy(probabilities, base=2)  # Base 2 for bits\n",
    "\n",
    "  return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to caclulate the <u>productivity</u> of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def sentence_productivity_calc(words, tags):\n",
    "  \"\"\"Calculates the productivity of a sentence based on word and tag data.\n",
    "\n",
    "  Args:\n",
    "    words: A list of words in the sentence.\n",
    "    tags: A list of corresponding POS tags.\n",
    "\n",
    "  Returns:\n",
    "    The productivity of the sentence.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate joint entropy H(W, T)\n",
    "  word_tag_pairs = list(zip(words, tags))\n",
    "  pair_counts = {}\n",
    "  for pair in word_tag_pairs:\n",
    "    pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "  total_pairs = sum(pair_counts.values())\n",
    "  probabilities = [count / total_pairs for count in pair_counts.values()]\n",
    "  H_WT = stats.entropy(probabilities, base=2)\n",
    "\n",
    "  # Calculate entropy of tags H(T)\n",
    "  tag_counts = {}\n",
    "  for tag in tags:\n",
    "    tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "  total_tags = sum(tag_counts.values())\n",
    "  tag_probabilities = [count / total_tags for count in tag_counts.values()]\n",
    "  H_T = stats.entropy(tag_probabilities, base=2)\n",
    "\n",
    "  # Calculate conditional entropy H(W | T)\n",
    "  H_WT_given_T = H_WT - H_T\n",
    "\n",
    "  # Calculate productivity\n",
    "  productivity = H_WT_given_T + 1\n",
    "\n",
    "  return productivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the document complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_complexity_calc(sentences, doc):\n",
    "    \"\"\"Calculates the document complexity, diversity, and productivity.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences (each sentence as a list of words).\n",
    "        doc: The entire document object.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - Document complexity\n",
    "        - Average document diversity\n",
    "        - Average document productivity\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(sentences)\n",
    "    total_complexity = 0\n",
    "    total_diversity = 0\n",
    "    total_productivity = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sen_words = []\n",
    "        sen_pos = []\n",
    "\n",
    "        for word in sentence.words:\n",
    "            if word.upos != \"PUNCT\":\n",
    "                sen_words.append(word.text.lower())\n",
    "                sen_pos.append(word.xpos)\n",
    "\n",
    "        # Calculate D(si) and P(si) for the current sentence\n",
    "        diversity = sentence_diversity_calc(sen_pos)\n",
    "        productivity = sentence_productivity_calc(sen_words, sen_pos)\n",
    "\n",
    "        # Update totals\n",
    "        total_complexity += diversity * productivity\n",
    "        total_diversity += diversity\n",
    "        total_productivity += productivity\n",
    "\n",
    "    # Calculate averages\n",
    "    document_complexity = total_complexity / N\n",
    "    average_diversity = total_diversity / N\n",
    "    average_productivity = total_productivity / N\n",
    "\n",
    "    return document_complexity, average_diversity, average_productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 07:21:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeff414f0c04ce48e7c68e2ba79c6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 07:21:45 INFO: Downloaded file to C:\\Users\\admin\\stanza_resources\\resources.json\n",
      "2024-10-16 07:21:45 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-16 07:21:45 INFO: Using device: cpu\n",
      "2024-10-16 07:21:45 INFO: Loading: tokenize\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-16 07:21:46 INFO: Loading: mwt\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-16 07:21:46 INFO: Loading: pos\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-16 07:21:47 INFO: Loading: lemma\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-16 07:21:47 INFO: Loading: constituency\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\constituency\\base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-16 07:21:47 INFO: Loading: depparse\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-16 07:21:47 INFO: Loading: sentiment\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\classifiers\\trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-16 07:21:48 INFO: Loading: ner\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\ner\\trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-16 07:21:48 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3950db5072574d8da13b67c505f21dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h1>Construction Complexity Calculator</h1>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae5ff318b044090a38e3482c9953ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p>This tool calculates the complexity measure described in Nelson (2024).</p>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d465779cfa8f40b4abe66dacb8e60b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p>For multiple texts, click \"upload\" and select the files. For one text, paste it into the box. Wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eff3068307943d594e5cd7a1ec24ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.txt', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3c08d5cba74260a449be8ee2ea1180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Input text:', layout=Layout(height='300px', width='600px'), placeholder='Copy â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785d6cbe127842d2ba2318ee0c8a27bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process Input', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6579f414adfb450bb77fca7e9308a9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa52212388564683bdd1763d5c01e413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<p>Read the Open Access paper for full details about the complexity measure:</p>\\n<p>Nelson, R. â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165a765ffb4b47089ee0564ff1f053f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Progress:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import stanza\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize the Stanza pipeline\n",
    "nlp = stanza.Pipeline(\"en\")\n",
    "\n",
    "# Create a title widget\n",
    "title_widget = widgets.HTML(value=\"<h1>Construction Complexity Calculator</h1>\")\n",
    "\n",
    "# Create a description widget\n",
    "description_widget = widgets.HTML(value=\"<p>This tool calculates the complexity measure described in Nelson (2024).</p>\")\n",
    "\n",
    "# Create a combined instructions widget\n",
    "instructions_widget = widgets.HTML(value=\"<p>For multiple texts, click \\\"upload\\\" and select the files. For one text, paste it into the box. When your texts are ready, click \\\"Process Input\\\".</p>\")\n",
    "\n",
    "# Create a file upload widget\n",
    "file_upload_widget = widgets.FileUpload(\n",
    "    accept=\".txt\",  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=True  # True to accept multiple files upload else False\n",
    ")\n",
    "\n",
    "# Create a text box widget\n",
    "text_box_widget = widgets.Textarea(\n",
    "    placeholder='Copy and paste text here',\n",
    "    description='Input text:',\n",
    "    layout=widgets.Layout(height='300px', width='600px')\n",
    ")\n",
    "\n",
    "# Create an output widget to display the result\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Create a progress bar widget\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='Progress:',\n",
    "    bar_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Define the function to process the input files and create a CSV file\n",
    "def process_input(files, text):\n",
    "    if files:\n",
    "        results = []\n",
    "        total_files = len(files)\n",
    "        progress_bar.max = total_files\n",
    "        \n",
    "        for i, uploaded_file in enumerate(files):\n",
    "            content = bytes(uploaded_file['content']).decode('utf-8')\n",
    "            doc = nlp(content)\n",
    "            complexity, avg_diversity, avg_productivity = document_complexity_calc(doc.sentences, doc)\n",
    "            results.append({'filename': uploaded_file['name'],\n",
    "                            'complexity': complexity,\n",
    "                            'diversity': avg_diversity,\n",
    "                            'productivity': avg_productivity})\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.value = i + 1\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        csv_filename = 'document_complexity_scores.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        output_widget.clear_output()\n",
    "        output_widget.append_stdout(f\"Complexity scores saved to {csv_filename}\")\n",
    "        progress_bar.value = 0  # Reset progress bar\n",
    "    \n",
    "    elif text:\n",
    "        progress_bar.max = 1\n",
    "        progress_bar.value = 0\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        complexity, avg_diversity, avg_productivity = document_complexity_calc(doc.sentences, doc)\n",
    "        output_widget.clear_output()\n",
    "        output_widget.append_stdout(f\"\"\"\n",
    "        Complexity score: {complexity}\n",
    "        Diversity: {avg_diversity}\n",
    "        Productivity: {avg_productivity}\n",
    "        \"\"\")\n",
    "        \n",
    "        progress_bar.value = 1  # Complete progress bar\n",
    "        progress_bar.value = 0  # Reset progress bar\n",
    "\n",
    "# Attach a button to process the input\n",
    "process_button = widgets.Button(description=\"Process Input\")\n",
    "process_button.on_click(lambda b: process_input(file_upload_widget.value, text_box_widget.value))\n",
    "\n",
    "# Create a reference widget with a hyperlink\n",
    "reference_widget = widgets.HTML(value=\"\"\"\n",
    "<p>Read the Open Access paper for full details about the complexity measure:</p>\n",
    "<p>Nelson, R. (2024). Using constructions to measure developmental language complexity. Cognitive Linguistics. <a href=\"https://doi.org/10.1515/cog-2023-0062\">https://doi.org/10.1515/cog-2023-0062</a></p>\n",
    "\"\"\")\n",
    "\n",
    "# Display the widgets (no centering for button)\n",
    "display(title_widget, description_widget, instructions_widget, file_upload_widget, text_box_widget, process_button, output_widget, reference_widget, progress_bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
